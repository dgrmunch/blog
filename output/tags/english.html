<!DOCTYPE html>
<html lang="fr">
  <head>
    <meta charset="utf-8">
    
    <title>xmunch.com  - english</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="xmunch.com : english">
    <meta property="og:title" content="xmunch.com - english" />
    <meta property="og:type" content="website" />
    <meta property="og:url" content="http://filedn.com/lzKR35HlP1yHLFtV0oLTAku/blog/tags/english.html" />
    <meta property="og:description" content="xmunch.com : english" />
    <meta property="og:locale" content="es_ES" />
    <meta property="og:site_name" content="xmunch.com" />
    <link href="../css/yeti/bootstrap.min.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link href="../css/asciidoctor.css" rel="stylesheet">
    <link href="../css/xmunch.css?build=@10" rel="stylesheet">
    <!-- link href="/css/bootstrap-theme.min.css" rel="stylesheet" -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href='http://alexgorbatchev.com/pub/sh/3.0.83/styles/shCore.css' rel='stylesheet' type='text/css'/>
    <link href='http://alexgorbatchev.com/pub/sh/3.0.83/styles/shThemeDefault.css' rel='stylesheet' type='text/css'/>

    <script>
              (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
                                                                        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
                                                                        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
                                                                        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

                                                                        ga('create', 'UA-62113801-1', 'auto');
                                                                        ga('send', 'pageview');
      </script>

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <!-- Fav and touch icons -->
    <!--<link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="../assets/ico/apple-touch-icon-57-precomposed.png">-->
    <link rel="shortcut icon" href="../favicon.ico">
  </head>
  <body>
    <div id="wrap">


	      <!-- Fixed navbar -->
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
        <div class="container-fluid">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle" onclick="if(document.getElementById('navbar-menu').classList.contains('collapse')) document.getElementById('navbar-menu').classList.remove('collapse'); else document.getElementById('navbar-menu').classList.add('collapse')">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <span class="navbar-brand"><img src="http://filedn.com/lzKR35HlP1yHLFtV0oLTAku/landings/sciartlab.com/index_files/sciarlab-inverted.png" alt="SciArt Lab"></span>
          </div>
          <div class="collapse navbar-collapse" id="navbar-menu">
            <ul class="nav navbar-nav">
              <li><a href="../archive.html">Archive</a></li>
              <li><a href="../tags/spanish.html">Art&iacute;culos en Espa&ntilde;ol</a></li>
              <li><a href="../tags/english.html">Posts in English</a></li>
              <li><a href="../feed.xml">RSS</a></li>
            </ul>
          </div><!--/.nav-collapse -->
        </div>
      </div>
      <div class="container">


	<div class="page-header">
            <div class="row">
                <div class="col-xs-12 col-md-8"><h1>Tag: english</h1></div>
            </div>
	</div>

    <div class="row">

    <div class="col-sm-8">
        
            

                <a href="../blog/2017/building-a-distributed-metaverse.html"><h1>Building a Lab in a Distributed Metaverse.</h1></a>

								<div class="blogpost">
                <p>08 December 2017</p>

                <p>Tags :
                <a href="programming.html">programming</a>, <a href="metaverse.html">metaverse</a>, <a href="webvr.html">webvr</a>, <a href="vr.html">vr</a>, <a href="code.html">code</a>, <a href="coding.html">coding</a>, <a href="sciart.html">sciart</a>, <a href="aframe.html">aframe</a>, <a href="english.html">english</a>, <a href="open-source.html">open-source</a>, <a href="distributed.html">distributed</a>, <a href="p2p.html">p2p</a>, <a href="decentraland.html">decentraland</a>, <a href="decentralized.html">decentralized</a>
                </p>

								<a href="https://twitter.com/share" class="twitter-share-button" data-url="http://xmunch.com/me/blog/2017/building-a-distributed-metaverse.html" data-text="Building a Lab in a Distributed Metaverse." data-via="dgrmunch" data-lang="fr">[Share in Twitter]</a>
								<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
								<div class="g-plusone" data-size="medium" data-href="http://xmunch.com/me/blog/2017/building-a-distributed-metaverse.html"></div>

                <p><!-- toc disabled -->
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>The Metaverse is a collective virtual shared space, created by the convergence of virtually enhanced physical reality and physically persistent virtual space, including the sum of all virtual worlds, augmented reality, and the internet. The word metaverse is a portmanteau of the prefix "meta" (meaning "beyond") and "universe" and is typically used to describe the concept of a future iteration of the internet, made up of persistent, shared, 3D virtual spaces linked into a perceived virtual universe.</p>
</div>
<div class="paragraph">
<p>The term was coined in Neal Stephenson&#8217;s 1992 science fiction novel Snow Crash, where humans, as avatars, interact with each other and software agents, in a three-dimensional space that uses the metaphor of the real world. Stephenson used the term to describe a virtual reality-based successor to the Internet</p>
</div>
<div class="paragraph">
<p><em>Extracted from <a href="https://en.wikipedia.org/wiki/Metaverse">Wikipedia</a></em></p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_towards_a_virtual_reality">Towards a Virtual Reality</h3>
<div class="paragraph">
<p>During the last year we have been exploring how the <a href="http://sciartlab.com">SciArt Lab</a> could contribute to the emergence of the distributed metaverse. We had the chance of testing new virtual reality open-source technologies and develop several <code>WebVR</code> components with a triple goal:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Create our own immersive experiments in virtual reality</strong> for phenomenological and artistic explorations.</p>
</li>
<li>
<p><strong>Contribute to the upcoming 3D Web</strong>, sharing content and open-source components with the idea of a future distributed metaverse in mind (built on the top of <code>a-frame</code> + <code>ipfs</code>).</p>
</li>
<li>
<p><strong>Build the facilities of the SciArt Lab in a virtual space</strong>, in order to have a place always accessible, independently of our location in the physical world.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Some of those experiments have been already released in social media. However, it may be useful to summarize at least some of our last achievements in a blog post.</p>
</div>
<div class="paragraph">
<p>The potential of <code>WebVR</code> is amazing. It leads to new opportunities for developers and content creators, fixing the problem of interoperability between three-dimensional environments by bringing the community together around web standards (compatible with any virtual reality hardware and accessible from any web browser).</p>
</div>
<div class="paragraph">
<p>Regarding the role of "decentralization" in the co-production of a metaverse, we should mention that <strong>the decentralized web is becoming achievable in the short term</strong>. If we take into account the evolution of protocols and technologies for distributed consensus or data storage, we can not deny that the dream of a <em>distributed</em> (<a href="https://dgrmunch.github.io/blog/blog/2017/blockchain.html">completely decentralized</a>) virtual reality is easier than ever before.</p>
</div>
<div class="paragraph">
<p>The use of new <em>peer-to-peer</em> (P2P) protocols such as <code>IPFS</code>, in conjunction with the <code>Ethereum</code> network or other blockchain-based systems, would make the creation of alternative societies, distributed and transnational, a plausible possibility. As I have been defending in the last decade, P2P technologies are empowering tools which may reshape our societies, and the keystone to implement a cyberspace able to guarantee open innovation and knowledge production.
If we combine the potential of <em>peer-to-peer</em> with the immersive experience that virtual reality can provide, we may obtain a mindblowing outcome: a world-wide <strong>distibuted metaverse</strong>.</p>
</div>
</div>
<div class="sect2">
<h3 id="_sciart_lab_metaverse_components">SciArt Lab Metaverse Components</h3>
<div class="paragraph">
<p>The following video displays part of the capabilities of our initial experiments in terms of immersive graphics and interaction with gaze controlled events and teleporting.</p>
</div>
<iframe width="100%" height="315" id="video0-vr" src="https://www.youtube.com/embed/AGB7ZwePYLM" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
<div class="paragraph">
<p>Additionally, we have been exploring new possibilities for music creation in a 3D virtual world, along with other projects related with music and technology.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
To read about other music-related projects (not related with VR) you can check the articles <a href="https://dgrmunch.github.io/blog/blog/2017/digital-music-creation.html">Digital creation for hackers and makers: prototyping musical tools from the SciArt Lab</a> and <a href="https://dgrmunch.github.io/blog/blog/2017/alda-tabs.html">Alda-tabs: Domain Specific Language for Guitar Players in the Java Virtual Machine</a>.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>So far, our experiments combining music + VR are straightforward <code>a-frame</code> components which may evolve within the context of more specific projects.</p>
</div>
<div class="paragraph">
<p>The following tweets are some examples of the prototypes we have been developing lately. The code will be updated in Github  (in <a href="https://github.com/dgrmunch">my repo</a> or <a href="https://github.com/sciartlab">the one of the SciArt Lab</a>).</p>
</div>
<div class="row">
<div class="col-sm-6">
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Controlling sound samples in the browser with <a href="https://twitter.com/aframevr?ref_src=twsrc%5Etfw">@aframevr</a>, <a href="https://twitter.com/hashtag/diy?src=hash&amp;ref_src=twsrc%5Etfw">#diy</a> electronics and the <a href="https://twitter.com/hashtag/webAudioAPI?src=hash&amp;ref_src=twsrc%5Etfw">#webAudioAPI</a><br>Soon: musical instruments in virtual reality! <a href="https://t.co/K66Yda4Pov">pic.twitter.com/K66Yda4Pov</a></p>&mdash; Diego González (@dgrmunch) <a href="https://twitter.com/dgrmunch/status/929883277204193280?ref_src=twsrc%5Etfw">November 13, 2017</a></blockquote>
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Finishing gaze-controlled flying scene for the <a href="https://twitter.com/sciartlab?ref_src=twsrc%5Etfw">@sciartlab</a> <a href="https://twitter.com/hashtag/metaverse?src=hash&amp;ref_src=twsrc%5Etfw">#metaverse</a> <br>Possible thanks to the <a href="https://twitter.com/aframevr?ref_src=twsrc%5Etfw">@aframevr</a> community.<a href="https://twitter.com/hashtag/googlecardboard?src=hash&amp;ref_src=twsrc%5Etfw">#googlecardboard</a> <a href="https://twitter.com/hashtag/WebVR?src=hash&amp;ref_src=twsrc%5Etfw">#WebVR</a> <a href="https://t.co/ZJxPh8KkUt">pic.twitter.com/ZJxPh8KkUt</a></p>&mdash; Diego González (@dgrmunch) <a href="https://twitter.com/dgrmunch/status/924723165330276352?ref_src=twsrc%5Etfw">October 29, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>
<div class="col-sm-6">
<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Testing our a-oscillator primitive:<br>bring <a href="https://twitter.com/hashtag/music?src=hash&amp;ref_src=twsrc%5Etfw">#music</a> to <a href="https://twitter.com/hashtag/virtualReality?src=hash&amp;ref_src=twsrc%5Etfw">#virtualReality</a> with the SciArt Lab Metaverse components &amp; <a href="https://twitter.com/aframevr?ref_src=twsrc%5Etfw">@aframevr</a><a href="https://twitter.com/hashtag/synth?src=hash&amp;ref_src=twsrc%5Etfw">#synth</a> <a href="https://twitter.com/hashtag/webaudioapi?src=hash&amp;ref_src=twsrc%5Etfw">#webaudioapi</a> <a href="https://t.co/tYUkcVWMTy">pic.twitter.com/tYUkcVWMTy</a></p>&mdash; SciArt Lab (@sciartlab) <a href="https://twitter.com/sciartlab/status/929885400809771009?ref_src=twsrc%5Etfw">November 13, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div></div>
</div>
<div class="sect2">
<h3 id="_from_the_sciart_lab_metaverse_components_to_decentraland">From the SciArt Lab Metaverse components to Decentraland</h3>
<div class="paragraph">
<p>After some of the <em>SciArt Lab Metaverse components</em> were published in <a href="https://github.com/dgrmunch">Github</a> and <a href="http://twitter.com/sciartlab">Twitter</a>, we were mentioned a couple of times in the <a href="https://aframe.io/">Week of A-Frame</a> series (supported by <a href="https://www.mozilla.org">Mozilla</a>).</p>
</div>
<div class="paragraph">
<p>Some weeks later, <strong>we were contacted by the team of <a href="http://decentraland.com">Decentraland</a> offering support for our project</strong>. That was a really exciting new, a really good chance for the near future of the SciArt Lab.</p>
</div>
<div class="paragraph">
<p><a href="http://www.decentraland.com">Decentraland</a> is one of the most promising projects of the new blockchain-based startup ecosystem. They <strong>raised $24 million</strong> during their <em>ICO</em>. <a href="https://www.forbes.com/sites/omribarzilay/2017/11/30/how-blockchain-is-breathing-new-life-into-virtual-real-estate/#49b9529f511e">Forbes wrote an article about them</a> recently, remarking how new economies may emerge very soon in virtual worlds.</p>
</div>
<div class="paragraph">
<p>Decentraland combines the possibilities of open standards, <code>WebVR</code> and decentralized technologies within a conceptual and economic framework: shared ownership of the platform.</p>
</div>
<div class="paragraph">
<p>Here there is an introduction from their website:</p>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p><strong>Decentraland is a virtual reality platform powered by the Ethereum blockchain. Users can create, experience, and monetize content and applications.</strong></p>
</div>
<div class="paragraph">
<p>Decentraland is the first virtual platform owned by its users.
Grab a VR headset or use your web browser and become completely immersed in a 3D, interactive world.
There are plenty of opportunities to explore or even create your own piece of the universe. Here, you can purchase land through the Ethereum blockchain, creating an immutable record of ownership. No one can limit what you build.</p>
</div>
<div class="paragraph">
<p>With full control over your land, you can create unique experiences unlike anything in existence. Your imagination is the limit: go to a casino, watch live music, attend a workshop, shop with friends, start a business, test drive a car, visit an underwater resort, and much, much more—all within a 360-degree, virtual world.</p>
</div>
</div>
</div>
<div class="paragraph">
<p>And this is a promotional video of the project:</p>
</div>
<iframe width="100%" height="315" id="video1-vr" src="https://www.youtube.com/embed/-HmXrOTEmxg" frameborder="0" gesture="media" allow="encrypted-media" allowfullscreen></iframe>
<div class="paragraph">
<p>After talking with them, we agreed to collaborate and build during 2018 the <em>SciArt Lab Metaverse Branch</em> as a district in Decentraland. They will provide the resources that we need to make this happen, so this opens a new period for the lab, in which patronage and partnerships will make our projects more sustainable.</p>
</div>
<div class="paragraph">
<p>We have recently initiated a partnership with <a href="http://maggarquitectura.com">Magg Architecture</a> for the construction of our facility in the metaverse.
More information about the evolution of the project will be published soon.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://dgrmunch.github.io/blog//img/sciartlabHeadquarters.jpg" alt="sciartlabHeadquarters"></span></p>
</div>
</div></p>
							</div>
            

        
            

                <a href="../blog/2017/digital-music-creation.html"><h1>Digital creation for hackers & makers: prototyping musical tools from the SciArt Lab.</h1></a>

								<div class="blogpost">
                <p>01 June 2017</p>

                <p>Tags :
                <a href="programming.html">programming</a>, <a href="midi.html">midi</a>, <a href="music.html">music</a>, <a href="code.html">code</a>, <a href="coding.html">coding</a>, <a href="sciart.html">sciart</a>, <a href="guitar.html">guitar</a>, <a href="alda-tabs.html">alda-tabs</a>, <a href="english.html">english</a>, <a href="open-source.html">open-source</a>, <a href="software.html">software</a>, <a href="ableton.html">ableton</a>
                </p>

								<a href="https://twitter.com/share" class="twitter-share-button" data-url="http://xmunch.com/me/blog/2017/digital-music-creation.html" data-text="Digital creation for hackers & makers: prototyping musical tools from the SciArt Lab." data-via="dgrmunch" data-lang="fr">[Share in Twitter]</a>
								<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
								<div class="g-plusone" data-size="medium" data-href="http://xmunch.com/me/blog/2017/digital-music-creation.html"></div>

                <p><div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
One of the main goals of the <a href="http://sciartlab.com">SciArt Lab</a> is the open exploration of innovative ideas from a <strong>maker/hacker perspective</strong>, finding innovation through prototyping rather than relying on mere theoretical approaches. In that sense, we try to combine <strong>disruptive technologies</strong> and <strong>scientific knowledge</strong> with unconventional developments and real implementations/prototypes of our ideas. If you want to know more about the research of the <strong>SciArt Lab</strong> check <a href="http://sciartlab.com">our website</a>.
</td>
</tr>
</table>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p><strong>What is this article about?</strong></p>
</div>
<div class="paragraph">
<p>This article is an introduction to some of the projects which have been developed by the <a href="http://sciartlab.com">SciArt Lab</a> around topics related with <strong>digital musical creation</strong>.</p>
</div>
<div class="paragraph">
<p>In this post I will summarize part of my <em>hands-on</em> experience based on the intersection of <em>DIY</em> electronics, <code>MIDI</code> controllers, and the development of new tools (coded in <code>Java</code>, <code>Groovy</code>, <code>Processing</code>, <code>Javascript</code>) in combination with existing software such as <em>Ableton Live</em>.</p>
</div>
<div class="paragraph">
<p>This is an on-going exploration, so  <a href="http://twitter.com/sciartlab">follow us on Twitter</a> and keep updated in the near future.</p>
</div>
</div>
</div>
<!-- toc disabled -->
<div class="sect2">
<h3 id="_music_and_digital_creation">Music and digital creation</h3>
<div class="paragraph">
<p>I can summarize the current projects of the <em>SciArt Lab</em> as a set of fun experiments.</p>
</div>
<div class="paragraph">
<p>Basically, we are <em>hacking music</em> with both <strong>sound synthesis</strong>, <code>MIDI</code> <strong>experiments</strong>, <strong>DIY electronics</strong> and <strong>algorithmic composition</strong>, combining experimental music with brand new technologies. Discovering how coding and music can be combined by prototyping domain-specific languages, enabling self-composed songs with genetic algorithms or re-discovering <code>MIDI</code> controllers to create audio art.</p>
</div>
<div class="sect3">
<h4 id="_a_genetic_algorithms_mathematical_compositions_and_generative_music">A. Genetic algorithms, mathematical compositions and generative music</h4>
<div class="paragraph">
<p>We are exploring the potential of applying <em>Artificial Intelligence</em> techniques and software development to create programs which are able to write and play their own music.</p>
</div>
<div class="paragraph">
<p>Take a look of our first experiments, <a href="https://www.youtube.com/playlist?list=PL7xg0jqeMOn8xBEn4tn5kuOjmHitTMZyZ">watching our videos</a> of <em>cellular automata</em> with emergent properties for music composition.</p>
</div>
<div class="paragraph">
<p>Each <em>cellular automaton</em> is able to compose its own music based on simple rules, evolving while it plays synthetic instruments in <em>Ableton Live</em> or external devices through <code>MIDI</code> events.</p>
</div>
<iframe width="100%" height="450" scrolling="no" src="https://www.youtube.com/embed/videoseries?list=PL7xg0jqeMOn8xBEn4tn5kuOjmHitTMZyZ" frameborder="0" allowfullscreen></iframe>
</div>
<div class="sect3">
<h4 id="_b_domain_specific_languages">B. Domain Specific Languages</h4>
<div class="paragraph">
<p><em>Alda-tabs</em> is the first <em>Domain Specific Language for Guitar Players in the Java Virtual Machine</em>. This piece of software can help guitar players to “execute” their music notes in the JVM, write songs and get audio feedback with basic tab syntax.
You can read more about this in <a href="https://dgrmunch.github.io/blog/blog/2017/alda-tabs.html">this article</a>.</p>
</div>
<div class="paragraph">
<p>Take a look of the potential of <em>Alda-tabs</em> with chords and arpeggios listening this example (code also provided):</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./alda-tabs.sh examples/01-guitartabs-example.alda</pre>
</div>
</div>
<div class="audioblock music-box">
<div class="content">
<audio src="https://dgrmunch.github.io/blog/songs/examples/example1.mp3" controls>
Your browser does not support the audio tag.
</audio>
</div>
</div>
<div class="paragraph">
<p><a href="https://raw.githubusercontent.com/dgrmunch/alda-tabs/master/examples/01-guitartabs-example.alda" class="fa fa-file code" target="_blank" rel="noopener"> Read alda-tabs code</a></p>
</div>
<hr>
</div>
<div class="sect3">
<h4 id="_c_digital_instruments_and_physical_interfaces">C. Digital instruments and physical interfaces</h4>
<div class="paragraph">
<p>A couple of years ago, when I was working as Visiting Researcher at the <em>Critical Making Lab</em> (<em>University of Toronto</em>), I discovered how a humanistic-based approach to <em>DIY electronics</em>, <em>coding</em> and <em>making</em> could change forever my conception on research.
That experience helped me to see the importance of hands-on learning and the role that <em>tangible objects</em> could have for theoretical or intellectual explorations.</p>
</div>
<div class="paragraph">
<p>Currently I am working on a prototype of a physical <code>MIDI</code> interface to control digital instruments directly from a guitar fret.
This same concept will be explored with different objects and materials (conductive or not) in the following months.</p>
</div>
<div class="paragraph">
<p>The idea is to go beyond the keyboard as the standard digital music interface and build physical <code>MIDI</code> controllers with wood, cardboard, fabric, etc.
More details about this project will be published soon.</p>
</div>
<div class="paragraph">
<p>In the meantime, I have been also testing some libraries of <code>Javascript</code> for sound synthesis, and playing around with <code>p5.js</code> to develop the foundations of <code>SoundBox</code>, an experimental digital environment for synthetic music creation. Basically, the idea with this tool is to transform a human voice or an instrument (through a microphone) in a <code>MIDI</code>
 interface. Right now, it basically detects the <em>fundamental frequency</em> of the microphone&#8217;s sound signal, allowing the user to transform a continuous signal in a set of discrete notes. It also parses that information and reproduces the played sequence in <em>Sin Oscillator</em>.</p>
</div>
<div class="paragraph">
<p>It is a very straightforward prototype with troubles with some harmonics, but it has been a good experiment to learn about how these issues work. Let&#8217;s see, but maybe <code>SoundBox</code> is the starting point for something more sophisticated.</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="https://raw.githubusercontent.com/SciArtLab/soundbox/master/img/img2.png" alt="img2"></span></p>
</div>
</div>
<div class="sect3">
<h4 id="_d_music_visualization">D. Music Visualization</h4>
<div class="paragraph">
<p>One of the research interests of the <em>SciArt Lab</em> is information visualization in unusual ways. I always was fascinated about <em>synesthesia</em> and lately I have been testing visual approaches to music. The idea with some of the prototypes I have been working in is to map <code>MIDI</code> inputs both with physical visualizations (i.a. LEDs) and computational ones.</p>
</div>
<div class="paragraph">
<p>In this second category, I have been testing the possibility of creating my own 2D graphics with <code>Processing</code> and <code>SVG</code> and animate them while controling their movements and shapes directly from external <code>MIDI</code> inputs.
This is one example of one program/animation that I have implemented recently:</p>
</div>
<iframe width="100%" height="450"  src="https://www.youtube.com/embed/W3ou3hSYBB8" frameborder="0" allowfullscreen></iframe>
<div class="paragraph">
<p>In the previous example, an animation is created dynamically in <code>Processing</code> while the behavior of an animated cartoon responds to the inputs received from an external <em>DIY electronics</em> device. Both the graphics and the sound are produced by the orders received through <code>MIDI</code> inputs.</p>
</div>
</div>
<div class="sect3">
<h4 id="_e_postsynaptic_symphonies">E. Postsynaptic Symphonies</h4>
<div class="paragraph">
<p>I have always liked music. I started writing songs with a keyboard as a kid and continued with a guitar when I was a teenager. Nowadays, I enjoy playing several kinds of instruments. Besides the keyboard, my acoustic guitar and my wife&#8217;s classical guitar, I have two harmonicas, some flutes, an ocarina, an ukulele and a guitalele.</p>
</div>
<div class="paragraph">
<p>Recently, as part of the open-ended exploration of the <em>SciArt Lab</em>, I have been writing also some digital music. I call them <em>postynaptic symphonies</em> because I found interesting the cognitive experience of listening that sort of unpredictable songs.</p>
</div>
<div class="paragraph">
<p>I have published some <code>postynaptic symphonies</code> in <em>SoundCloud</em>:</p>
</div>
<iframe width="100%" height="450" scrolling="no" frameborder="no" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/playlists/322369717&amp;auto_play=false&amp;hide_related=false&amp;show_comments=true&amp;show_user=true&amp;show_reposts=false&amp;visual=true"></iframe>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p>More information about the evolution of my <em>music-related projects</em> will be coming soon :)</p>
</div>
</div>
</div>
</div>
</div></p>
							</div>
            

        
            

                <a href="../blog/2017/alda-tabs.html"><h1>Alda-tabs: Domain Specific Language for Guitar Players in the Java Virtual Machine</h1></a>

								<div class="blogpost">
                <p>13 April 2017</p>

                <p>Tags :
                <a href="english.html">english</a>, <a href="open-source.html">open-source</a>, <a href="music.html">music</a>, <a href="code.html">code</a>, <a href="jvm.html">jvm</a>, <a href="sciart.html">sciart</a>, <a href="guitar.html">guitar</a>, <a href="alda-tabs.html">alda-tabs</a>, <a href="software.html">software</a>
                </p>

								<a href="https://twitter.com/share" class="twitter-share-button" data-url="http://xmunch.com/me/blog/2017/alda-tabs.html" data-text="Alda-tabs: Domain Specific Language for Guitar Players in the Java Virtual Machine" data-via="dgrmunch" data-lang="fr">[Share in Twitter]</a>
								<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
								<div class="g-plusone" data-size="medium" data-href="http://xmunch.com/me/blog/2017/alda-tabs.html"></div>

                <p><div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
This post explains how to easily compose music with <code>alda-tabs</code>, a <strong>Domain Specific Language for Guitar Players</strong> which runs in the JVM. I have developed <code>alda-tabs</code> as an <em>open source</em> project so you can download it for free in <a href="https://github.com/dgrmunch/alda-tabs">GitHub</a>.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_what_is_alda_tabs">What is alda-tabs?</h3>
<div class="ulist">
<ul>
<li>
<p>It is a Domain Specific Language for guitar players.</p>
</li>
<li>
<p>It is a piece of software to help guitar players to
"execute" their music notes in the <code>JVM</code>, compose songs and get audio feedback.</p>
</li>
<li>
<p>It is an extensible tool for music programming mainly
oriented to guitar players.</p>
</li>
<li>
<p>It is built on the top of <code>Alda</code>, a DSL for
music composition in the JVM, so it is compatible with both <code>Alda</code> and <code>Clojure</code>.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_why_is_so_easy_to_code_guitar_songs_with_alda_tabs">Why is so easy to code guitar songs with alda-tabs?</h3>
<div class="ulist">
<ul>
<li>
<p>It does not require programming skills.</p>
</li>
<li>
<p>It does not require traditional music notation.</p>
</li>
<li>
<p>It is as straightforward as writing simple guitar sketches in a
notebook.</p>
</li>
<li>
<p>You only have to copy your tabs from the paper to a text editor and
execute <code>alda-tabs</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><span class="image white-img"><img src="https://raw.githubusercontent.com/dgrmunch/alda-tabs/master/img/alda-tabs.png" alt="alda tabs"></span></p>
</div>
</div>
<div class="sect2">
<h3 id="_how_can_i_create_complex_digital_music_with_alda_tabs">How can I create complex digital music with alda-tabs?</h3>
<div class="ulist">
<ul>
<li>
<p>With <code>alda-tabs</code> you can execute any <em>.alda</em> file, so you can write
your songs/programs in both <code>Clojure</code>, <code>Alda</code> and <code>alda-tabs</code> in the same
block of text.</p>
</li>
<li>
<p>It talks to the <code>JVM</code>, so any experimented programmer can do impossible
things :-)</p>
</li>
<li>
<p>It is just a layer on the top of <code>Alda</code>, so if you know music theory,
then you can write complex songs using music notation.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_how_is_the_alda_tabs_syntax">How is the alda-tabs syntax?</h3>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
Remember that with <strong>alda-tabs</strong> you can always use the standard
Alda syntax and Clojure code. You can learn more about both languages
later to explore the whole potential of <em>alda-tabs</em>. But don&#8217;t worry,
you don&#8217;t need to know more yet. Just follow this tutorial and enjoy :)
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>What I am gonna show you here is the easy and super simple <em>alda-tabs</em>
syntax:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <strong>tab notation</strong></p>
</li>
<li>
<p>The <strong>chord notation</strong></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In 10 minutes you will be able to write songs in a text editor and
listen the result in your speakers.</p>
</div>
<div class="sect3">
<h4 id="_tab_notation">Tab notation</h4>
<div class="paragraph">
<p>Imagine that you want to play all the strings of the guitar, one after
another:</p>
</div>
<div class="paragraph">
<p><span class="image white-img"><img src="https://raw.githubusercontent.com/dgrmunch/alda-tabs/master/img/score1.png" alt="score1"></span></p>
</div>
<div class="paragraph">
<p>This example is a regular <em>guitar tab</em> in which all the strings are played sequentially with one hand and in which there are not fingers of the other hand pressing the fret.</p>
</div>
<div class="paragraph">
<p><strong>So the fret number would be <code>0</code> in the six positions of the sequence.</strong></p>
</div>
<div class="exampleblock">
<div class="content">
<div class="paragraph">
<p><strong>How would we write this in <code>Alda</code> syntax?</strong></p>
</div>
<div class="paragraph">
<p><em>Don&#8217;t worry, I will explain how to do it in <code>alda-tabs</code> syntax bellow (see <a href="#how-to-do-it-in-alda-tabs">How to do it in alda-tabs?</a>) but it is important to read this before to compare <code>alda-tabs</code> with</em> <code>Alda</code>.</p>
</div>
<div class="paragraph">
<p>In <code>Alda</code> syntax we would need to know the note equivalents of each position. And in addition:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>We would write the octave and the note, one after another:</p>
</li>
</ul>
</div>
<div class="literalblock">
<div class="content">
<pre>guitar: o4 e o3 b o3 g o3 d o2 a o2 e</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Another way would be to write the initial octave increasing/decreasing it
when needed:</p>
</li>
</ul>
</div>
<div class="literalblock">
<div class="content">
<pre>guitar: o4 e/&gt;b/g/d/&lt;&lt;a/e</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect3">
<h4 id="how-to-do-it-in-alda-tabs">How to do it in alda-tabs?</h4>
<div class="paragraph">
<p>Remember that <em>alda-tabs</em> is based in the simple concept of a tab.
Basically, the notes of a guitar can be defined by numeric combinations,
a number to identify the string (from the first at the bottom to the
sixth at the top) and a fret.</p>
</div>
<div class="paragraph">
<p>To write a note in <em>alda-tabs</em> you only have to write <code>ta</code> followed by
the <code>string number</code> and the <code>fret number</code>.</p>
</div>
<div class="paragraph">
<p>With <code>alda-tabs</code> we can write the same sequence that we have previously
expressed in <code>Alda</code>. But this time we don&#8217;t need to know which note we are
playing, we only need to write the <em>tab</em>, the position of our finger
considering the <em>string</em> and the <em>fret</em>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre> guitar: ta10 ta20 ta30 ta40 ta50 ta60</pre>
</div>
</div>
<div class="paragraph">
<p>In this example, we are asking the JMV to play a guitar with the open strings 1, 2, 3, 4, 5 and 6, one after another. That is, <code>ta10</code> equals <strong>string 1</strong> and <strong>fret 0</strong> and so on.</p>
</div>
<div class="paragraph">
<p>Take a look now of the fretboard:</p>
</div>
<div class="paragraph">
<p><span class="image white-img"><img src="https://raw.githubusercontent.com/dgrmunch/alda-tabs/master/img/notes-fret.png" alt="notes fret"></span></p>
</div>
<div class="paragraph">
<p>If you want to play the first note <strong>C</strong>, according to the graphic displayed above, you don&#8217;t need to know the octave, you just will pick the <strong>string 2</strong> in the <strong>fret 1</strong>: <code>ta</code> + <code>2</code> + <code>1</code>.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>guitar: ta21</pre>
</div>
</div>
<div class="paragraph">
<p>You can also modify the duration of a note adding a character at the
end. For example:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>guitar: ta21 ta21W ta21Q ta21D ta21H</pre>
</div>
</div>
<div class="paragraph">
<p>What does it mean? If you don&#8217;t specify a duration, this will be <strong>whole
beat</strong> (<code>W</code>). You can also play the note during half beat (<code>H</code>), double (<code>D</code>)
and quarter (<code>Q</code>). Those note durations will be proportional to the tempo
of the score. For example, the following two sentences are not the same:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>(tempo 100)
guitar: ta21 ta21W ta21Q ta21D ta21H</pre>
</div>
</div>
<div class="literalblock">
<div class="content">
<pre>(tempo 300)
guitar: ta21 ta21W ta21Q ta21D ta21H</pre>
</div>
</div>
<div class="paragraph">
<p><em>Play with these combinations to see the difference</em>. For more complex
timing, check the <em>advanced tips</em> bellow.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="chord-notation">Chord notation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Imagine that rather than a sequence of notes you want to play a chord. A
basic example would be playing all the open strings at the same time:</p>
</div>
<div class="paragraph">
<p><span class="image white-img"><img src="https://raw.githubusercontent.com/dgrmunch/alda-tabs/master/img/score2.png" alt="score2"></span></p>
</div>
<div class="paragraph">
<p>You can do this in three ways:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In <code>Alda</code> syntax, using the character <code>/</code> to play the notes at the same
time:</p>
</li>
</ul>
</div>
<div class="literalblock">
<div class="content">
<pre>guitar: o4 e/&gt;b/g/d/&lt;&lt;a/e</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>In <code>alta-tabs</code> syntax, using the <em>tab notation</em> with different voices:</p>
</li>
</ul>
</div>
<div class="literalblock">
<div class="content">
<pre>guitar: V1: ta10 V2: ta20 V3: ta30 V4: ta40 V5: ta50 V6: ta60</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>In <code>alta-tabs</code> syntax, but using the <strong>chord notation</strong>:</p>
</li>
</ul>
</div>
<div class="literalblock">
<div class="content">
<pre>guitar: (c 0 0 0 0 0 0 W)</pre>
</div>
</div>
<div class="paragraph">
<p>As you can see, the chord notation is just a <code>Clojure</code> function <code>c</code> with
seven parameters, the fret of each one of the six strings and the
duration of the chord.</p>
</div>
<div class="paragraph">
<p>For example, the D chord would be</p>
</div>
<div class="literalblock">
<div class="content">
<pre>(c 2 3 2 0 x x W)</pre>
</div>
</div>
<div class="paragraph">
<p><span class="image white-img"><img src="https://raw.githubusercontent.com/dgrmunch/alda-tabs/master/img/re.png" alt="re"></span></p>
</div>
<div class="paragraph">
<p>You can also use the <em>chord notation</em> to play single notes. For example,
the two following sequences are exactly the same:</p>
</div>
<div class="literalblock">
<div class="content">
<pre># alta-tab syntax

ta10 ta20 ta30 ta40 ta50 ta60

# alda-tab chord syntax

(c 0 x x x x x W)
(c x 0 x x x x W)
(c x x 0 x x x W)
(c x x x 0 x x W)
(c x x x x 0 x W)
(c x x x x x 0 W)</pre>
</div>
</div>
<div class="sect2">
<h3 id="advanced-tips">Advanced tips</h3>
<div class="paragraph">
<p>You can play tabs with specific durations, in seconds or milliseconds by
using the function <code>t</code>. In this case you should write the number of
the string, followed by a dot and the fret. Add the end, you should
express in String format ("") the duration you want.</p>
</div>
<div class="literalblock">
<div class="content">
<pre>guitar: (t 1.0 "2s") (t 2.0 "10ms") (t 2.2 "100ms")</pre>
</div>
</div>
<div class="paragraph">
<p>You can do the same with your chords:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>guitar: (c 0 x 1 2 2 x "5s")</pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="how-can-i-install-alta-tabs">How can I install alta-tabs?</h3>
<div class="ulist">
<ul>
<li>
<p>Follow the <a href="https://github.com/alda-lang/alda">steps to install Alda</a>.</p>
</li>
<li>
<p>Clone <a href="https://github.com/dgrmunch/alda-tabs">this repo</a> and open the folder <code>alda-tabs</code> in your terminal.</p>
</li>
<li>
<p>Run the Alda server with <code>alda up</code>.</p>
</li>
<li>
<p>Create a simple text file, write your song using the alda-tabs
syntax and save it.</p>
</li>
<li>
<p>Execute <code>./alda-tabs.sh</code> followed by the path of the file you want
to play.</p>
</li>
<li>
<p>Listen the result.</p>
</li>
<li>
<p>If you want to stop a song you can stop the alda server with
<code>alda down</code>.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
You can also play some scores (provided in the <code>/examples</code>
folder) and modify their content to explore different sounds.
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_examples">Examples</h2>
<div class="sectionbody">
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
In this document you can both read the code and listen the output of its execution in <code>alda-tabs</code>. However, the provided audio file is <strong>non-stereo</strong>. The original output of <code>alda-tabs</code>, however, includes <code>panning</code>. So explore the real result executing the code/song in your own instance of <code>alda-tabs</code>.
</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_example_1_chords_and_arpeggios">Example 1: Chords and arpeggios</h3>
<div class="paragraph">
<p>You can start exploring the potential of <em>alda-tabs</em> with chords and arpeggios
with the <strong>example #01</strong>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./alda-tabs.sh examples/01-guitartabs-example.alda</pre>
</div>
</div>
<div class="audioblock music-box">
<div class="content">
<audio src="https://dgrmunch.github.io/blog/songs/examples/example1.mp3" controls>
Your browser does not support the audio tag.
</audio>
</div>
</div>
<div class="paragraph">
<p><a href="https://raw.githubusercontent.com/dgrmunch/alda-tabs/master/examples/01-guitartabs-example.alda" class="fa fa-file code" target="_blank" rel="noopener"> Read alda-tabs code</a></p>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_example_2_the_sound_of_pi">Example 2: The sound of Pi</h3>
<div class="paragraph">
<p>Try to compose <em>mathematical songs</em> extending <em>Alda</em> and <em>alda-tabs</em>
with <em>Clojure</em>. See the <strong>example #02</strong>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./alda-tabs.sh examples/02-pi.alda</pre>
</div>
</div>
<div class="audioblock music-box">
<div class="content">
<audio src="https://dgrmunch.github.io/blog/songs/examples/example2.mp3" controls>
Your browser does not support the audio tag.
</audio>
</div>
</div>
<div class="paragraph">
<p><a href="https://raw.githubusercontent.com/dgrmunch/alda-tabs/master/examples/02-pi.alda" class="fa fa-file code" target="_blank" rel="noopener"> Read alda-tabs code</a></p>
</div>
<hr>
</div>
<div class="sect2">
<h3 id="_example_3_complex_songs">Example 3: Complex songs</h3>
<div class="paragraph">
<p>You can also see how beautiful songs with multiple instruments can be
written with Alda with the <strong>example #03</strong>:</p>
</div>
<div class="literalblock">
<div class="content">
<pre>./alda-tabs.sh examples/03-hope-for-future-ext.alda</pre>
</div>
</div>
<div class="audioblock music-box">
<div class="content">
<audio src="https://dgrmunch.github.io/blog/songs/examples/example3.mp3" controls>
Your browser does not support the audio tag.
</audio>
</div>
</div>
<div class="paragraph">
<p><a href="https://raw.githubusercontent.com/dgrmunch/alda-tabs/master/examples/03-hope-for-future-ext.alda" class="fa fa-file code" target="_blank" rel="noopener"> Read alda-tabs code</a></p>
</div>
</div>
</div>
</div></p>
							</div>
            

        
            

                <a href="../blog/2017/rhizome.html"><h1>A new research: Introduction to Rhizome Ethnographies</h1></a>

								<div class="blogpost">
                <p>01 March 2017</p>

                <p>Tags :
                <a href="english.html">english</a>, <a href="ethnography.html">ethnography</a>, <a href="multiculturalism.html">multiculturalism</a>, <a href="data-visualization.html">data-visualization</a>, <a href="diversity.html">diversity</a>
                </p>

								<a href="https://twitter.com/share" class="twitter-share-button" data-url="http://xmunch.com/me/blog/2017/rhizome.html" data-text="A new research: Introduction to Rhizome Ethnographies" data-via="dgrmunch" data-lang="fr">[Share in Twitter]</a>
								<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
								<div class="g-plusone" data-size="medium" data-href="http://xmunch.com/me/blog/2017/rhizome.html"></div>

                <p><div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<div class="title">Note</div>
</td>
<td class="content">
This article was originally published in the <a href="http://www.xmunch.com/rhizome">Rhizome Ethnographies</a> site. I have republished it here as an introduction for those who don&#8217;t know the project yet.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>After several years exploring, from a computational perspective, the evolution of social complexity in heterogeneous and decentralized <em>agent-based models</em> <a href="#1">[1]</a> <a href="#2">[2]</a>, I have decided to start a new stage of research. With this side project, I would like to study the evolution of identity/identities in real social agents. Specifically exploring heterogeneous societies through the observation of their ethnic, linguistic and cultural contexts, and using visual ethnography to learn about transnational identities and multiculturalism.</p>
</div>
<div class="paragraph">
<p>So let’s say that I am moving from the fields of <strong>Computer Science and Information Science</strong> to a new multidisciplinary approach closer to the domains of <strong>Sociology and Cultural Anthropology</strong>. From software simulations of <em>artificial agents</em> to <strong>multicultural communities</strong> of actual people. From <em>genetic algorithms</em> to <em>GIS</em> analysis, data visualization, maps, sociology and unstructured interviews.</p>
</div>
<div id="img-map" class="imageblock">
<div class="content">
<img src="https://dgrmunch.github.io/blog/img/map.png" alt="map">
</div>
</div>
<div class="paragraph">
<p>So far, most of my research has been based in quantitative approaches, predominantly <em>in-silico</em> modeling of bacteria colonies and <em>artificial societies</em> <a href="#3">[3]</a>. I have been studying the evolution of <em>artificial societies</em> with <strong>Artificial Intelligence</strong> techniques, with the goal of finding answers to Information Science questions <a href="#4">[4]</a>. In fact, most of my former research was oriented, under the umbrella of the <strong>Complex Adaptive Systems</strong> paradigm (CAS), to understand the role of decentralization and heterogeneity in the emergence of social complexity in innovative scenarios <a href="#5">[5]</a> <a href="#6">[6]</a>.  Regarding the application of my research to Social Sciences, you can tell that most of it was mainly grounded on computational and mathematical models. Fortunately, that lack of actual data can be addressed by radically changing my approach.</p>
</div>
<div class="paragraph">
<p>During my graduate studies, I coined (and re-coined) a couple of terms (such as “P2P Society” or “P2P Paradigm”) trying to understand the social dynamics of a globalized and plural world. A world with fully decentralized communication networks (see <strong>distributed topology</strong> <a href="#6">[6]</a>) and collaborative communities of heterogeneous peers. I also did some side research on distributed technologies such as different <em>blockchain-based</em> projects to frame the P2P society in that growing ecosystem of emancipatory tools and methods. The <strong>Bitmind</strong> project <a href="#7">[7]</a> was one of those initiatives.</p>
</div>
<div class="paragraph">
<p>I had the chance to exchange knowledge with several scholars from different fields, trying to do multidisciplinary research. But I was always constrained by the limits of my own field and its methodologies. So even though I will continue publishing and doing research in my own field, I would like to explore in parallel a different experience.</p>
</div>
<div class="paragraph">
<p>In my thesis and the published papers, posters and communications, I always try to be faithful and accurate, and to keep my research as much constrained as possible in order to offer reliable results according to the scientific method. However, sometimes I feel that to explore social issues properly I need something else, something more than just a quantitative and hypothetical-deductive approach. So I decided that it was the time for a new kind of experience, an open-ended side project, a new kind of both personal and academic approach to reality. Less closed and constrained, less <em>machine-ish</em> and more <em>human-ish</em>.</p>
</div>
<div class="paragraph">
<p>Although I had a great time observing the evolution of different social dynamics with a computationally-based approach, I realized that I wanted to know more about the human being as a social construct. And I wanted to know it from my own subjective point of view, the point of view of another social construct.</p>
</div>
<div class="paragraph">
<p>Considering the term ‘agent’ as the most convenient to refer to free individuals endowed with reason and autonomy, I can say that this time I am focused in the observation of the agent itself, a common individual within a physical and social environment. And I am doing it with a completely different toolbox. I want to conduct this research with my own ontological and epistemological constraints, but without the ones imposed by an specific academic field. Understand the inner nature of the social agent, and maybe study again the functional and structural evolution of the social object (the social network of agents). But obviously, with a very different approach. This approach shall be less deterministic, objective, quantitative and pretentious, while focusing on the live experience rather than on the expected results. I will accomplish this by going to the arena and getting immersed rather than implementing a synthetic <em>in-silico</em> arena and observing it through a screen, doing visual ethnography and taking field notes rather than coding simulations and analyzing results. If you are more interested in the other perspective, you can check my peer-reviewed publications and follow my other projects.</p>
</div>
<div class="paragraph">
<p>One of the first steps to make this change happen is to avoid a hypothetical-deductive mindset and assume a more inductive approach, without preconceived axioms, without a priori hypotheses or pretentious irrefutable truths. In order to do that I have to go beyond my academic background, unlearn and relearn again, and start the new stage without self-constraints. With a methodology (or a set of methodologies) but without a fixed and self-constraining outline.</p>
</div>
<div class="paragraph">
<p>Computational social modeling requires a reduced amount of variables in isolation and a considerable lack of sociological, anthropological, historical or psychological background. Ethnographical research, however, requires a very different mindset. It may imply critical thinking, personal immersion and opened eyes in the seeking of facts, qualitative data and other kinds of knowledge.</p>
</div>
<div class="paragraph">
<p>Furthermore, self-conducted research can also be <strong>phenomenological</strong>. And in addition to that, it may imply a bunch of other things. It can be a rhizome of field notes, secondary data, memories and media resources. It can be a mesh of interconnected experiences.</p>
</div>
<div id="img-bus" class="imageblock">
<div class="content">
<img src="https://farm2.staticflickr.com/1443/26621997606_86dda84ab8_b.jpg" alt="26621997606 86dda84ab8 b">
</div>
</div>
<div class="paragraph">
<p>Considering this new stage as a chosen path for knowledge crawling, I would consider it both research, game and art. Besides that, naturally, I would also consider this project a way of self-exploration.</p>
</div>
<div class="quoteblock">
<blockquote>
<div class="paragraph">
<p>What defines the self besides the constraints of national boundaries, ethnicity, race, religion and culture? Are we just a consequence of our context and history? Would it be possible to understand better the complexity of individual and group identities combining photography and film-making as visual ethnography tools with other traditional methods? Could we generalize what we learn from a small sample of people to understand better the notions of ethnicity, race and cultural identity? What can we discover about the object of study with a phenomenological perspective?</p>
</div>
</blockquote>
</div>
<div class="paragraph">
<p>This research follows an <strong>ethnographical approach</strong> and eventually may use some traditional methods, such as participant and direct observation <a href="#8">[8]</a> or unstructured interviewing, in order to look for possible answers to the above-mentioned questions. However, it is mainly a <strong>visual ethnography</strong> and it uses photography and film-making as visual languages. It is also a phenomenological research. It is also a live experience without hypotheses and pre-conceived axioms. It is a happy lack of expectation.</p>
</div>
<div class="paragraph">
<p>If you want to know more about the project visit the <a href="http://www.xmunch.com/rhizome">Rhizome Ethnographies website.</a></p>
</div>
<div class="sect1">
<h2 id="_references">REFERENCES</h2>
<div class="sectionbody">
<div class="paragraph">
<p>[[[1]]] J. R. Hernandez-Carrion and D. González-Rodriguez, “Modelling Complex Dynamics and Distributed Generation of Knowledge with Bacterial-Based Algorithms,” Scientific Publications / University of Economics in Katowice, vol. Economics and Business Communication Challenges : International Week, pp. 138–148, 2014.</p>
</div>
<div class="paragraph">
<p>[[[2]]] Gonzalez-Rodriguez, Diego and Hernandez-Carrion, Jose Rodolfo, “Decentralization and heterogeneity in complex adaptive systems,” Kybernetes, vol. 44, no. 6/7, pp. 1082–1093, Jun. 2015.</p>
</div>
<div class="paragraph">
<p>[[[3]]] D. González Rodríguez, “Modelización de Sistemas de Computación Distribuida con Bacterias Sintéticas mediante Autómatas Celulares,” 2014.</p>
</div>
<div class="paragraph">
<p>[[[4]]] D. Gonzalez-Rodriguez and J. R. Hernandez-Carrion, “Fundamentos teóricos básicos para la defensa de un paradigma P2P cooperativo desde la perspectiva de la producción de conocimiento,” Revista Iberoamericana de Autogestión y Acción Comunal (RIDAA), vol. 0, no. 66, pp. pp. 55–66, 2015.</p>
</div>
<div class="paragraph">
<p>[[[5]]] D. Gonzalez-Rodriguez and J. R. Hernandez-Carrion, “A Bacterial-Based Algorithm to Simulate Complex Adaptive Systems,” in From Animals to Animats 13, A. P. del Pobil, E. Chinellato, E. Martinez-Martin, J. Hallam, E. Cervera, and A. Morales, Eds. Springer International Publishing, 2014, pp. 250–259.</p>
</div>
<div class="paragraph">
<p>[[[6]]] D. Gonzalez-Rodriguez and V. Kostakis, “Information literacy and peer-to-peer infrastructures: An autopoietic perspective,” Telematics and Informatics, vol. 32, no. 4, pp. 586–593, Nov. 2015.</p>
</div>
<div class="paragraph">
<p>[[[7]]] Gonzalez-Rodriguez, Diego and Shapiro, Ishan, “Scalable cognition through collaborative sense-making: drafting the Open Value Network model,” presented at the Workshop on Synthetic Cognitive Development and Integrated-Distributed Agency (IDA). The Eighth Conference on Artificial General Intelligence, Berlin, 2015.</p>
</div>
<div class="paragraph">
<p>[[[8]]] B. B. Kawulich, “Participant Observation as a Data Collection Method,” Forum Qualitative Sozialforschung / Forum: Qualitative Social Research, vol. 6, no. 2, May 2005.</p>
</div>
</div>
</div></p>
							</div>
            

        
            

                <a href="../blog/2017/blockchain.html"><h1>Blockchain Technologies and the P2P Paradigm</h1></a>

								<div class="blogpost">
                <p>03 February 2017</p>

                <p>Tags :
                <a href="english.html">english</a>, <a href="commons.html">commons</a>, <a href="blockchain.html">blockchain</a>, <a href="p2p.html">p2p</a>
                </p>

								<a href="https://twitter.com/share" class="twitter-share-button" data-url="http://xmunch.com/me/blog/2017/blockchain.html" data-text="Blockchain Technologies and the P2P Paradigm" data-via="dgrmunch" data-lang="fr">[Share in Twitter]</a>
								<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script>
								<div class="g-plusone" data-size="medium" data-href="http://xmunch.com/me/blog/2017/blockchain.html"></div>

                <p><p><img src="https://dgrmunch.github.io/blog/img/blockchain.jpg" alt="" /></p>
<h2>Blockchain, Commons and P2P</h2>
<p>Michel Bauwens defines the term <strong>P2P</strong> as the <em>relational dynamic of distributed networks</em>.</p>
<p><em>Distributed networks</em> are different from <em>centralized networks</em>, those with a centralized topology on one node. They are also different from the <em>decentralized or federated networks</em>, those with several central nodes connected to each other. In other words, distributed networks would be those whose nodes could be connected without intermediates, exchanging information freely without central control.</p>
<p>Considering this, we can tell that the concept behind the acronym <em>P2P</em> is strongly related with the topology of our communication infrastructures. But it also implies a change in our practices and values, and may lead to a change of paradigm with very important implications in a wide variety of fields such as economics, politics, science or education.</p>
<p>However, when we talk about the <em>Blockchain</em> we are talking basically about a distributed ledger of transactions and the protocol which enables different nodes to use the same ledger. In other words, a protocol that allows agents to be part of the same transaction network by <strong>building consensus</strong> with <em>cryptographic algorithms</em>.</p>
<p>We can describe the <em>Blockchain-related technologies</em> as one revolutionary step towards the development of fully decentralized (that is, distributed) infrastructures. Furthermore, we can consider that the biggest implication of these new technological frameworks is that they allow something else that just information exchange through distributed networks. Rather, they enable distributed production and flow of value across the Internet.</p>
<p>Coming back to the initial argument (and seen the big picture of the <em>P2P paradigm</em>), <em>Blockchain technologies</em> may imply something else than just a new <strong>distributed infrastructure</strong>. They may imply the emergence of new organization models for <em>P2P interaction</em>. If a <em>blockchain-based software</em> can enable <strong>distributed production and flow of value</strong>, then it can be used to bootstrap the value production and distribution capabilities of <strong>Commons Based Peer Production</strong> (CBPP) communities. Or maybe it can be used exactly for the opposite, to increase control mechanisms or preserve more competition-based political and economic views, like <em>crypto-anarchism</em> or <em>libertarianism</em>. Talking about technology always should imply a critical perspective, and that is one of the main goals of <em>STS</em>.</p>
<p>An optimistic point of view can lead us to consider that the <em>Blockchain hype</em> is kind of a <em>panacea</em>.</p>
<p>It can be a tipping point to reach the dreamed <strong>P2P Society</strong>, leading to the minimization of strategic homogeneity, as much as the maximization of the decentralization of political and economic structures. It can imply a higher emergence of <em>collective intelligence</em> and <em>self-organization</em>, enabling new <strong>peer-to-peer social dynamics</strong>, and being a support for new models of knowledge production within the <em>CBPP</em> approach.</p>
<p>However, it is obvious that science and technology should be discussed with a critical eye (and not necessarily by defending any kind of <em>neo-luddism</em>). Assuming that critical mindset and discussing technological breakthroughs from a <em>commons-oriented perspective</em>, may lead to entrepreneurial initiatives focused on the improvement of our communities. Emphasizing the idea of combining <em>Blockchain-enabled</em> technologies with the production goals of <em>P2P paradigm</em>, the project <a href="http://www.sciartlab.com/?page_id=186">Bitmind</a> was born in 2015 as a <strong>commons-oriented startup</strong>.</p>
<h2>Bitmind and the Open Value Networks</h2>
<p>I founded <strong>Bitmind</strong>, in collabotation with Vasilis Kostakis <strong>(P2P Lab)</strong> and Ishan Shapiro <strong>(Metamaps)</strong>, as a decentralized entrepreneurial initiative whose goal was the exploration of the potential of <strong>Open Value Networks (OVN)</strong> by using <em>blockchain-based technologies</em>.</p>
<p>It was founded in early 2015 as a side entrepreneurial project, looking for an application of my doctoral dissertation. Bitmind was one of the initiatives funded during the <strong>Faircoop donations campaign in 2015</strong>.</p>
<p>The summer of 2015, Bitmind founders were invited to present the OVN model at the <em>AGI Conference 2015 (The eighth Conference on Artificial General Intelligence)</em>. The talk, <em>“Scalable cognition through collaborative sense-making: drafting the Open Value Network model”</em> was included in the <strong>Workshop on Synthetic Cognitive Development and Integrated-Distributed Agency (IDA)</strong>, organized by <em>David Weinbaum</em> and <em>Viktoras Veitas</em> from the <strong>Global Brain Institute (Free University of Brussels)</strong>. Our talk was introduced by <a href="https://en.wikipedia.org/wiki/Ben_Goertzel">Ben Goertzel</a>.</p>
<p>Some months later, after discussing with the teams of <strong>Sensorica</strong> and <strong>Backfeed</strong> about our common interests, we decided to focus our efforts in applied research &amp; open source development around the OVN/DCO context, exploring how new models for <em>self-organization</em> could enforce the value production capacities within the <em>P2P paradigm</em>. In order to unify concepts, share common methodologies and explore the field, we talked about the need of creating a shared hub for this evolving and growing ecosystem.</p>
<p>Bitmind’s initial goal was to help <strong>Open Enterprises, Cooperatives and Communities</strong> to distribute value amongst their members and bootstrap their <em>collective intelligence</em> by using P2P infrastructures. In order to unify concepts, share common methodologies and explore that field, we developed <em>OVN.SPACE</em>. This initiative enabled collaboration between different Open Value Networks, including companies and projects such as Sensorica, Backfeed, The Citizens Media, Mikorizal, Metamaps, Value Flows and Kendra Initiative.</p>
<p>In October 2015, a group of 35 designers, engineers, and entrepreneurs from all over the world gathered in Oakland, California. They group consisted of founders and key contributors from Enspiral, Loomio, CoBudget, Chalkle, Robin Hood Cooperative, Identity.com, Hylo, Ethereum, Citizen Code, Metamaps.cc, Bitmind, KiwiConnect, Lifehack, Planetwork, Impact Hub, Refugio Resource, Pyxis, Triaxiom9, CivicMakers and more. As a consequence of some days of exchange and dialogue about the creation of tools, strategies, and networks to build a new collaborative commons, the <strong>Collaborative Technology Alliance</strong> was created.</p>
<p>One month later, the <strong>P2P Lab (Tallinn University of Technology)</strong> included Bitmind as a <strong>commons-oriented start-up</strong> in its plans for 2016. After that, Bitmind members, with other collaborators of the <em>P2P Lab</em>, worked on proposals to explore blockchain technologies under the framework of the <em>European Union research goals.</em></p>
<p>I started to collaborate with Primavera de Filippi <strong>(Berkman Center for Internet &amp; Society at Harvard)</strong> and Vasilis Kostakis. And at the beginning of 2016, COALA (organizer the first <strong>Blockchain Workshops at Harvard, MIT and Stanford</strong>) invited me to participate in the Blockchain Workshop 2016 in New York City, to present Bitmind with other blockchain-based projects in front of potential investors.</p>
<h2>The transfer of Bitmind to the P2P Foundation</h2>
<p>The project was gaining momentum.</p>
<p>I started to receive job offers from blockchain-related enterprises like <strong>Visa Research</strong> and <strong>Coinbase</strong>. But I was tired of the entrepreneurial stress and the fast evolution of the Blockchain ecosystem. Companies such as Facebook or Amazon started to contact me.</p>
<p>That was too much, so I decided to take a step back.</p>
<p>I had just moved to North Carolina and had other plans for my life. I wanted to continue working remotely for <em>Enxendra Technologies</em> while developing my next personal initiatives. This time I wanted to unify all my research interests, but looking more to the creative process than to the market.</p>
<p>So I founded the <a href="http://sciartlab.com">SciArt Lab</a> and started as <em>Visiting Researcher</em> for the <strong>University of North Carolina</strong>, developing my new research interests.</p>
<p>I focused on other projects, initiating a process of coordination to transfer the Bitmind initiative to the <strong>P2P Foundation</strong>.</p>
<p>Since it was founded, Bitmind efforts were mostly in applied research &amp; open source development around the context of <em>Open Value Networks</em> and <em>Distributed Collaborative Organizations</em>, exploring how new models for self-organization could enforce the value production capacities within the P2P paradigm.</p>
<p>Bitmind was one of the first entrepreneurial initiatives oriented to unify CBPP governance and economic models with blockchain-based technologies, helping to initiate the conversation about new self-organizing infrastructures in value production communities.</p>
<p>In July 2016, Bitmind funds were transferred to the <em>P2P Foundation</em>.</p>
</p>
							</div>
            

        

    </div>

		
        <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
            <div class="avatar-box sidebar-module sidebar-module-inset">
                <h4>Diego González, PhD</h4>
                <p><b>Research & Development</b></p>
                <img src="../img/avatar.jpg"  height="128" width="128">
                <br><br>
                <a href="https://filedn.com/lzKR35HlP1yHLFtV0oLTAku/wiki/#!about.md">About me</a><br>
            </div>

            <div class="avatar-box sciart-lab-box sidebar-module sidebar-module-inset">
                <a href="http://www.sciartlab.com" target="blank_"><img src="http://filedn.com/lzKR35HlP1yHLFtV0oLTAku/landings/sciartlab.com/index_files/sciarlab-inverted.png" height="228"></a>
            </div>

            <div class="sidebar-module sidebar-module-inset">
              <b>Social Media / Redes sociales</b><br>

                <ul>
                    <li><a href="https://twitter.com/dgrmunch">Twitter</a></li>
										<li><a href="https://www.linkedin.com/in/xmunch/en">Linkedin</a></li>
										<li><a href="https://github.com/dgrmunch">Github</a></li>
										<li><a href="https://www.instagram.com/xmunch">Instagram</a></li>
										<li><a href="https://www.flickr.com/photos/xmunch">Flickr</a></li>
                </ul>
            </div>

            <div class="sidebar-module sidebar-module-inset">
							<b>Other sites / Otros sitios</b><br>
              <ul>
                  <li><a href="http://www.sciartlab.com">SciArt Lab</a></li>
							    <li><a href="http://xmunch.com/blog">Rhizome Ethnographies</a></li>
              </ul>
					  </div>

            <div class="sidebar-module">
                <h4>Tags</h4>
                <ol class="list-unstyled" style="margin-left: 0px">
                    
                        <li><a href="tags/english.html">english</a> (5)</li>
                    
                        <li><a href="tags/code.html">code</a> (3)</li>
                    
                        <li><a href="tags/open-source.html">open-source</a> (3)</li>
                    
                        <li><a href="tags/p2p.html">p2p</a> (3)</li>
                    
                        <li><a href="tags/sciart.html">sciart</a> (3)</li>
                    
                        <li><a href="tags/alda-tabs.html">alda-tabs</a> (2)</li>
                    
                        <li><a href="tags/coding.html">coding</a> (2)</li>
                    
                        <li><a href="tags/decentralization.html">decentralization</a> (2)</li>
                    
                        <li><a href="tags/diversity.html">diversity</a> (2)</li>
                    
                        <li><a href="tags/guitar.html">guitar</a> (2)</li>
                    
                        <li><a href="tags/multiculturalism.html">multiculturalism</a> (2)</li>
                    
                        <li><a href="tags/music.html">music</a> (2)</li>
                    
                        <li><a href="tags/programming.html">programming</a> (2)</li>
                    
                        <li><a href="tags/software.html">software</a> (2)</li>
                    
                        <li><a href="tags/spanish.html">spanish</a> (2)</li>
                    
                        <li><a href="tags/ableton.html">ableton</a> (1)</li>
                    
                        <li><a href="tags/aframe.html">aframe</a> (1)</li>
                    
                        <li><a href="tags/blockchain.html">blockchain</a> (1)</li>
                    
                        <li><a href="tags/commons.html">commons</a> (1)</li>
                    
                        <li><a href="tags/data-visualization.html">data-visualization</a> (1)</li>
                    
                        <li><a href="tags/decentraland.html">decentraland</a> (1)</li>
                    
                        <li><a href="tags/decentralized.html">decentralized</a> (1)</li>
                    
                        <li><a href="tags/distributed.html">distributed</a> (1)</li>
                    
                        <li><a href="tags/ethnography.html">ethnography</a> (1)</li>
                    
                        <li><a href="tags/ipfs.html">ipfs</a> (1)</li>
                    
                        <li><a href="tags/jvm.html">jvm</a> (1)</li>
                    
                        <li><a href="tags/metaverse.html">metaverse</a> (1)</li>
                    
                        <li><a href="tags/midi.html">midi</a> (1)</li>
                    
                        <li><a href="tags/postmodern.html">postmodern</a> (1)</li>
                    
                        <li><a href="tags/vr.html">vr</a> (1)</li>
                    
                        <li><a href="tags/webvr.html">webvr</a> (1)</li>
                    
                 </ol>
            </div>
        </div>


    </div>

		</div>
		<div id="push"></div>
    </div>

    <div id="footer">
      <div class="container">
        <p class="text-muted credit">&copy; Diego Gonzalez 2017 | <a href="http://xmunch.com">xmunch.com</a> | Baked with <a href="http://jbake.org">JBake v2.6.4</a></p>
      </div>
    </div>

  </body>
</html>

